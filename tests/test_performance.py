"""
Performance tests for large repository handling.
Tests memory usage, processing time, and scalability limits.
"""

import pytest
import time
import psutil
import sys
import os
from datetime import datetime, timedelta
from typing import List, Dict

# Add src to Python path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

from performance.benchmark_suite import MockDataGenerator, PerformanceBenchmark

class TestLargeRepositoryPerformance:
    """Test performance with large repositories."""
    
    @pytest.fixture
    def data_generator(self):
        """Fixture for mock data generator."""
        return MockDataGenerator()
    
    @pytest.fixture
    def benchmark(self):
        """Fixture for performance benchmark."""
        return PerformanceBenchmark()
    
    def test_memory_usage_scales_linearly(self, benchmark):
        """Test that memory usage scales reasonably with repository size."""
        # Test different sizes
        test_configs = [
            {'commits': 100, 'contributors': 5, 'files': 20},
            {'commits': 500, 'contributors': 15, 'files': 50},
            {'commits': 1000, 'contributors': 25, 'files': 100}
        ]
        
        memory_usages = []
        
        for i, config in enumerate(test_configs):
            result = benchmark._benchmark_repository_size(f"test_{i}", config)
            
            assert result.success, f"Benchmark failed: {result.error_message}"
            memory_usages.append(result.memory_usage_mb)
        
        # Memory usage shouldn't increase exponentially
        # Allow some overhead but shouldn't be more than 3x for 10x data
        assert memory_usages[2] < memory_usages[0] * 15, "Memory usage scaling is too high"
    
    def test_processing_time_reasonable(self, benchmark):
        """Test that processing time is reasonable for large repositories."""
        # Test with 5000 commits (large repo)
        config = {'commits': 5000, 'contributors': 50, 'files': 500}
        
        result = benchmark._benchmark_repository_size("large_test", config)
        
        assert result.success, f"Large repository test failed: {result.error_message}"
        
        # Should process at least 10 commits per second
        commits_per_second = result.commit_count / result.processing_time_seconds
        assert commits_per_second >= 10, f"Processing too slow: {commits_per_second:.2f} commits/sec"
        
        # Should not use more than 2GB of memory
        assert result.memory_usage_mb < 2048, f"Memory usage too high: {result.memory_usage_mb:.1f}MB"
    
    def test_10k_commits_benchmark(self, benchmark):
        """Test processing 10,000 commits (enterprise size)."""
        config = {'commits': 10000, 'contributors': 100, 'files': 1000}
        
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        result = benchmark._benchmark_repository_size("enterprise_test", config)
        
        if result.success:
            # Verify metrics are calculated correctly
            assert result.metrics_calculated is not None
            assert 0 <= result.metrics_calculated['team_health_score'] <= 1
            assert 0 <= result.metrics_calculated['sentiment_score'] <= 1
            
            # Performance expectations for 10K commits
            assert result.processing_time_seconds < 120, "Processing took too long (>2 minutes)"
            assert result.memory_usage_mb < 4096, "Memory usage too high (>4GB)"
            
            print(f"‚úÖ 10K commits processed in {result.processing_time_seconds:.2f}s")
            print(f"üìä Memory usage: {result.memory_usage_mb:.1f}MB")
        else:
            # For systems that can't handle 10K commits, this is expected
            print(f"‚ö†Ô∏è 10K commit test failed (expected on lower-end systems): {result.error_message}")
            assert "memory" in result.error_message.lower() or "timeout" in result.error_message.lower()
    
    def test_concurrent_repository_analysis(self, data_generator):
        """Test analyzing multiple repositories concurrently."""
        import threading
        from concurrent.futures import ThreadPoolExecutor
        
        # Create 3 small repositories
        repos = []
        for i in range(3):
            commits = data_generator.generate_commits(200)
            repos.append(commits)
        
        def analyze_repo(commits):
            """Simulate repository analysis."""
            start_time = time.time()
            
            # Simulate processing
            sentiments = [c['sentiment'] for c in commits]
            avg_sentiment = sum(sentiments) / len(sentiments)
            
            processing_time = time.time() - start_time
            
            return {
                'commits': len(commits),
                'sentiment': avg_sentiment,
                'time': processing_time
            }
        
        # Test concurrent processing
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [executor.submit(analyze_repo, repo) for repo in repos]
            results = [future.result() for future in futures]
        
        total_time = time.time() - start_time
        
        # Concurrent processing should be faster than sequential
        sequential_time_estimate = sum(r['time'] for r in results)
        
        assert total_time < sequential_time_estimate, "Concurrent processing not faster than sequential"
        assert len(results) == 3, "Not all repositories processed"
        
        print(f"üöÄ Concurrent: {total_time:.2f}s vs Sequential estimate: {sequential_time_estimate:.2f}s")
    
    def test_memory_leak_detection(self, data_generator):
        """Test for memory leaks during repeated analysis."""
        import gc
        
        initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        # Run multiple analyses
        for i in range(5):
            commits = data_generator.generate_commits(500)
            
            # Simulate analysis
            sentiments = [c['sentiment'] for c in commits]
            collaborations = {}
            
            for commit in commits:
                for file_path in commit['files_changed']:
                    if file_path not in collaborations:
                        collaborations[file_path] = set()
                    collaborations[file_path].add(commit['author'])
            
            # Force garbage collection
            del commits, sentiments, collaborations
            gc.collect()
        
        final_memory = psutil.Process().memory_info().rss / 1024 / 1024
        memory_increase = final_memory - initial_memory
        
        # Memory increase should be minimal (< 100MB)
        assert memory_increase < 100, f"Potential memory leak: {memory_increase:.1f}MB increase"
        
        print(f"üß† Memory increase after 5 iterations: {memory_increase:.1f}MB")
    
    def test_large_commit_message_handling(self, data_generator):
        """Test handling of very large commit messages."""
        # Generate commits with very long messages
        large_commits = []
        
        for i in range(100):
            commit = {
                'id': f'large_commit_{i}',
                'author': 'test@example.com',
                'date': datetime.now().isoformat(),
                'message': 'A' * 10000,  # 10KB message
                'files_changed': ['test.py'],
                'lines_added': 10,
                'lines_deleted': 5,
                'lines_changed': 15,
                'sentiment': 0.5
            }
            large_commits.append(commit)
        
        start_time = time.time()
        
        # Simulate sentiment analysis on large messages
        sentiments = []
        for commit in large_commits:
            # Simulate NLP processing (which might be slow with large text)
            time.sleep(0.001)  # 1ms processing time
            sentiments.append(commit['sentiment'])
        
        processing_time = time.time() - start_time
        
        # Should handle large messages efficiently
        assert processing_time < 5, f"Large message processing too slow: {processing_time:.2f}s"
        assert len(sentiments) == 100, "Not all large messages processed"
        
        print(f"üìù Processed 100 large messages in {processing_time:.2f}s")
    
    @pytest.mark.parametrize("commit_count", [1000, 2500, 5000])
    def test_scalability_across_sizes(self, benchmark, commit_count):
        """Test scalability across different repository sizes."""
        config = {
            'commits': commit_count,
            'contributors': max(5, commit_count // 100),
            'files': max(10, commit_count // 50)
        }
        
        result = benchmark._benchmark_repository_size(f"scale_test_{commit_count}", config)
        
        if result.success:
            # Calculate efficiency metrics
            time_per_commit = result.processing_time_seconds / result.commit_count
            memory_per_commit = result.memory_usage_mb / result.commit_count
            
            # Performance should be reasonable across all sizes
            assert time_per_commit < 0.1, f"Too slow: {time_per_commit:.4f}s per commit"
            assert memory_per_commit < 1.0, f"Too much memory: {memory_per_commit:.4f}MB per commit"
            
            print(f"üìà {commit_count:,} commits: {time_per_commit*1000:.2f}ms/commit, {memory_per_commit:.3f}MB/commit")
        else:
            # Log failure but don't fail test (system limitations)
            print(f"‚ö†Ô∏è {commit_count:,} commits test failed: {result.error_message}")
    
    def test_error_handling_with_malformed_data(self, benchmark):
        """Test error handling with malformed or edge case data."""
        # Test with empty repository
        empty_result = benchmark._simulate_analysis([], {'commits': 0, 'contributors': 0, 'files': 0})
        assert isinstance(empty_result, dict), "Should handle empty repository gracefully"
        
        # Test with commits missing required fields
        malformed_commits = [
            {'id': 'test1'},  # Missing most fields
            {'id': 'test2', 'author': 'test@example.com'},  # Missing date
        ]
        
        try:
            result = benchmark._simulate_analysis(malformed_commits, {'commits': 2, 'contributors': 1, 'files': 1})
            # Should either succeed with reasonable defaults or handle gracefully
            assert isinstance(result, dict), "Should return valid result even with malformed data"
        except Exception as e:
            # If it raises an exception, it should be a meaningful one
            assert "required" in str(e).lower() or "missing" in str(e).lower(), f"Unexpected error: {e}"
        
        print("üõ°Ô∏è Error handling test completed")

class TestPerformanceOptimizations:
    """Test specific performance optimizations."""
    
    def test_batch_processing_efficiency(self, data_generator=MockDataGenerator()):
        """Test that batch processing is more efficient than individual processing."""
        commits = data_generator.generate_commits(1000)
        
        # Test individual processing
        start_time = time.time()
        individual_results = []
        for commit in commits[:100]:  # Process first 100 individually
            sentiment = commit['sentiment']
            individual_results.append(sentiment)
        individual_time = time.time() - start_time
        
        # Test batch processing
        start_time = time.time()
        batch_sentiments = [c['sentiment'] for c in commits[:100]]
        batch_time = time.time() - start_time
        
        # Batch should be faster (though this is a simple example)
        efficiency_ratio = individual_time / batch_time if batch_time > 0 else float('inf')
        
        print(f"‚ö° Batch processing {efficiency_ratio:.2f}x faster")
        assert efficiency_ratio >= 1, "Batch processing should be at least as fast as individual"
    
    def test_memory_efficient_streaming(self, data_generator=MockDataGenerator()):
        """Test memory-efficient streaming processing of large datasets."""
        # Simulate streaming processing
        def process_commits_streaming(commits, batch_size=100):
            """Process commits in batches to save memory."""
            results = []
            
            for i in range(0, len(commits), batch_size):
                batch = commits[i:i+batch_size]
                
                # Process batch
                batch_sentiments = [c['sentiment'] for c in batch]
                results.extend(batch_sentiments)
                
                # Simulate memory cleanup
                del batch, batch_sentiments
            
            return results
        
        large_commits = data_generator.generate_commits(2000)
        
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        # Process with streaming
        streaming_results = process_commits_streaming(large_commits, batch_size=200)
        
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        memory_used = end_memory - start_memory
        
        assert len(streaming_results) == 2000, "All commits should be processed"
        assert memory_used < 500, f"Streaming should use less memory: {memory_used:.1f}MB"
        
        print(f"üåä Streaming processing used {memory_used:.1f}MB for 2000 commits")

if __name__ == "__main__":
    # Run specific performance tests
    pytest.main([__file__, "-v", "--tb=short"])